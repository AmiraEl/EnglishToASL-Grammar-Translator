{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This code takes in a sentence and parses it to the correct grammatical form then plays the correct videos needed"
      ],
      "metadata": {
        "id": "rs-SWWH9Vbi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import wordnet as wn\n",
        "import string\n",
        "import spacy\n",
        "import itertools\n",
        "import os"
      ],
      "metadata": {
        "id": "f9srfccyuV96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36F79EO8VVuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bfeb871-5fd1-4eb1-ae1a-b46424fcc625"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "# nlp = spacy.load('en_core_web_lg')  #takess longer to load but is a larger dictionary "
      ],
      "metadata": {
        "id": "QpdKfFWxSE3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timeOrderWords = ['aurora', 'afternoon','bedtime', 'break of day', 'break of the day', 'breakfast time','canonical hour','closing time',\n",
        " 'cockcrow','complin','compline','crepuscle','crepuscule','dawn','dawning','daybreak','dayspring','dinnertime',\n",
        " 'dusk','early morning hour','evenfall','evening','evensong','fall','first light','gloam','gloaming','happy hour','high noon','late night hour',\n",
        " 'lights out','lunch period','lunchtime','matins','mealtime','midday','midnight','morning','morning prayer','night','nightfall','none','nones','noon',\n",
        " 'noonday','noontide','prime','rush hour','sext','small hours','sundown','sunrise','sunset','sunup','suppertime','terce','tierce','twelve noon','twilight','vespers','zero hour']"
      ],
      "metadata": {
        "id": "ohZFMbLtXZ_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=[\"The big black cat stared at the small dog.\",\n",
        "           \"I won't watch her brother in the evenings.\",\n",
        "           \"Which car did Jane buy?\"]\n"
      ],
      "metadata": {
        "id": "VBMbyk6EtOj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translateToASL(sentence:list) -> list:\n",
        "  doc = nlp(sentence)\n",
        "  # for word in doc\n",
        "  tokens_list = []\n",
        "  signs_list = []\n",
        "  for token in doc:\n",
        "    # remove auxilaries, punctuation, determiners, prepositions\n",
        "      if token.pos_ not in ['AUX', 'PUNCT'] and token.tag_ not in ['DT','IN']:\n",
        "          tokens_list.append([token.text, token.lemma_, token.pos_, token.tag_,])\n",
        "          signs_list.append(token.lemma_)\n",
        "\n",
        "  for sign in signs_list:\n",
        "    signs_list[signs_list.index(sign)] = sign.lower()\n",
        "  # print(signs_list)\n",
        "\n",
        "  if 'not' in signs_list: \n",
        "    ind = signs_list.index('not')\n",
        "    signs_list[ind], signs_list[ind+1] = signs_list[ind+1], signs_list[ind]\n",
        "  # not comes after the verb it negates in ASL\n",
        "  # print(signs_list)\n",
        "\n",
        "  if 'i' in signs_list:\n",
        "    ind = signs_list.index('i')\n",
        "    signs_list[ind] = 'me'\n",
        "  # 'me' is used instead of 'i' in ASL\n",
        "  # print(signs_list)\n",
        "\n",
        "  for sign in signs_list:\n",
        "    if sign in timeOrderWords:\n",
        "      word = signs_list[signs_list.index(sign)]\n",
        "      signs_list.remove(word)\n",
        "      signs_list.insert(0, word)\n",
        "  # time order words need to be at the start of the sentence in ASL\n",
        "  return signs_list"
      ],
      "metadata": {
        "id": "0jIkhQP_dBXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in sentences: print(translateToASL(x)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yocPUfHTdfzB",
        "outputId": "55f7cebc-f4d1-42f1-ad62-7e977107a35e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['big', 'black', 'cat', 'stare', 'small', 'dog']\n",
            "['evening', 'me', 'watch', 'not', 'her', 'brother']\n",
            "['which', 'car', 'jane', 'buy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# timeOrderWords = [synset.lemma_names() for synset in wn.synsets('time_of_day')[0].hyponyms()]\n",
        "# for x in wn.synsets('time_of_day')[0].hyponyms():\n",
        "#   if len(x.hyponyms()) > 0:\n",
        "#     timeOrderwords = [timeOrderWords.append(y.lemma_names()) for y in x.hyponyms()]\n",
        "# timeOrderWords = list(itertools.chain(*timeOrderWords))\n",
        "# timeOrderWords = [timeOrderWords[x].replace('_', \" \") for x in range(len(timeOrderWords))]\n",
        "# timeOrderWords = [timeOrderWords[x].replace('-', \" \") for x in range(len(timeOrderWords))]\n",
        "# timeOrderWords"
      ],
      "metadata": {
        "id": "ZQpwpVT6Ne8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"\""
      ],
      "metadata": {
        "id": "HitrfxeuiRrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if word not found in videos, display letter by letter spelling\n",
        "def findfiles(directory:str, sentence:str) -> list:\n",
        "  sentence = translateToASL(sentence)\n",
        "  videos = list(os.listdir(directory))\n",
        "  for x in sentence:\n",
        "    if x+\".mp4\" not in videos:\n",
        "      temp = list(x)\n",
        "      sentence.insert(sentence.index(x), temp)\n",
        "      sentence.remove(x)\n",
        "\n",
        "  sentence = list(itertools.chain(*sentence))  #flatten list\n",
        "  sentence = [x+\".mp4\" for x in sentence] # add video name extension\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "Hi19ZfLv7mG1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}